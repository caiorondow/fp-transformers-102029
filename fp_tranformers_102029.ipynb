{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "TI-9zzptD63D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SFx4iuTqCCoS"
      },
      "outputs": [],
      "source": [
        "image_sz = 32\n",
        "batch_sz = 32\n",
        "channel_sz = 3\n",
        "patch_sz = 4\n",
        "hidden_sz = 512\n",
        "embed_sz = 512\n",
        "\n",
        "n_epochs = 25\n",
        "n_heads = 8\n",
        "n_layers = 4\n",
        "n_classes = 10\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WoKyrWhpQp2l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention block"
      ],
      "metadata": {
        "id": "d-u6bTPwC7bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHAttention(nn.Module):\n",
        "    def __init__(self, n_heads, embed_sz, dropout, batch_sz):\n",
        "        super(MHAttention, self).__init__()\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.embed_sz = embed_sz\n",
        "        self.head_sz = embed_sz // n_heads\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.Q = nn.Linear(self.embed_sz, self.embed_sz)\n",
        "        self.K = nn.Linear(self.embed_sz, self.embed_sz)\n",
        "        self.V = nn.Linear(self.embed_sz, self.embed_sz)\n",
        "        self.output_layer = nn.Linear(self.embed_sz, self.embed_sz)\n",
        "        self.dropout_layer = nn.Dropout(self.dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        q0, q1, q2 = q.size()\n",
        "        k0, k1, k2 = k.size()\n",
        "        v0, v1, v2 = v.size()\n",
        "\n",
        "        q = self.Q(q).reshape(q0, q1, self.n_heads, self.head_sz)\n",
        "        k = self.K(k).reshape(k0, k1, self.n_heads, self.head_sz)\n",
        "        v = self.V(v).reshape(v0, v1, self.n_heads, self.head_sz)\n",
        "\n",
        "        if self.batch_sz == 1:\n",
        "            q = q.transpose(0, 1)\n",
        "            k = k.transpose(0, 1)\n",
        "            v = v.transpose(0, 1)\n",
        "\n",
        "        attention = self.attention(q, k, v, mask)\n",
        "        return self.output_layer(attention.reshape(-1, v1, self.embed_sz))\n",
        "\n",
        "    def attention(self, q, k, v, mask=None):\n",
        "        scores = torch.einsum(\"bqhe,bkhe->bhqk\", [q, k])\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        scores = scores / math.sqrt(self.head_sz)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        scores = self.dropout_layer(scores)\n",
        "        return torch.einsum(\"bhql,blhd->bqhd\", [scores, v])"
      ],
      "metadata": {
        "id": "9-INQAH3C4ro"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "sgWcTlkEGQyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, n_heads, embed_sz, hidden_sz, dropout):\n",
        "        super(ViTEncoder, self).__init__()\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.embed_sz = embed_sz\n",
        "        self.hidden_sz = hidden_sz\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.embed_sz)\n",
        "        self.norm2 = nn.LayerNorm(self.embed_sz)\n",
        "        self.attention = MHAttention(n_heads, embed_sz, dropout, batch_sz=0)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.embed_sz, 4 * self.embed_sz),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(4 * self.embed_sz, self.embed_sz),\n",
        "            nn.Dropout(self.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.norm1(x), self.norm1(x), self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "M6TCuo4YGUjx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeiT"
      ],
      "metadata": {
        "id": "PqCdua7FIFZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeiT(nn.Module):\n",
        "    def __init__(self, image_sz, channel_sz, patch_sz, embed_sz, n_heads, n_layers, n_classes, hidden_sz, teacher_model, dropout):\n",
        "        super(DeiT, self).__init__()\n",
        "\n",
        "        self.image_sz = image_sz\n",
        "        self.channel_sz = channel_sz\n",
        "        self.patch_sz = patch_sz\n",
        "        self.embed_sz = embed_sz\n",
        "        self.hidden_sz = hidden_sz\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout = dropout\n",
        "        self.teacher_model = teacher_model\n",
        "\n",
        "        self.num_patches = (image_sz // patch_sz) ** 2\n",
        "        self.patch_sz_flat = channel_sz * (patch_sz ** 2)\n",
        "        self.embedding_layer = nn.Linear(self.patch_sz_flat, self.embed_sz)\n",
        "        self.class_token = nn.Parameter(torch.randn(1, 1, self.embed_sz))\n",
        "        self.distillation_token = nn.Parameter(torch.randn(1, 1, self.embed_sz))\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, self.num_patches + 2, self.embed_sz))\n",
        "        self.dropout_layer = nn.Dropout(self.dropout)\n",
        "\n",
        "        self.encoders = nn.ModuleList([\n",
        "            ViTEncoder(self.n_heads, self.embed_sz, self.hidden_sz, self.dropout)\n",
        "            for _ in range(self.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Linear(self.embed_sz, self.n_classes)\n",
        "\n",
        "        for param in self.teacher_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.teacher_model.eval()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        b, c, h, w = x.size()\n",
        "\n",
        "        teacher_logits = self.teacher_model(x)\n",
        "\n",
        "        x = x.view(b, self.num_patches, self.patch_sz_flat)\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        b, n, e = x.size()\n",
        "        class_tk = self.class_token.expand(b, -1, -1)\n",
        "        distillation_tk = self.distillation_token.expand(b, -1, -1)\n",
        "\n",
        "        x = torch.cat((class_tk, x, distillation_tk), dim=1)\n",
        "        x = self.dropout_layer(x + self.positional_encoding)\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "\n",
        "        x, distillation_token = x[:, 0, :], x[:, -1, :]\n",
        "        x = self.classifier(x)\n",
        "        return x, teacher_logits"
      ],
      "metadata": {
        "id": "0a3TAm6cIH9Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hard distillation loss"
      ],
      "metadata": {
        "id": "E0rOrctBRx3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HardDistillationLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HardDistillationLoss, self).__init__()\n",
        "\n",
        "        self.teacher_cel = nn.CrossEntropyLoss()\n",
        "        self.student_cel = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, teacher_y, student_y, y):\n",
        "        return 0.5 * ( (self.student_cel(student_y, y)) + (self.teacher_cel(teacher_y, y)) )"
      ],
      "metadata": {
        "id": "0e-v6AO2Rz_O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teacher"
      ],
      "metadata": {
        "id": "kxgrJylRM8Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "\n",
        "import math\n",
        "\n",
        "class VGG16_classifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 image_sz,\n",
        "                 n_classes,\n",
        "                 hidden_sz,\n",
        "                 dropout\n",
        "                 ):\n",
        "\n",
        "        self.image_sz = image_sz\n",
        "        self.n_classes = n_classes\n",
        "        self.hidden_sz = hidden_sz\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.vgg16 = models.vgg16(pretrained=True)\n",
        "        for parameter in self.vgg16.parameters():\n",
        "            parameter.requires_grad = True\n",
        "        self.vgg16.classifier = nn.Sequential(\n",
        "                nn.Linear(25088, self.hidden_sz * 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(self.dropout),\n",
        "                nn.Linear(self.hidden_sz * 4, self.hidden_sz * 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(self.dropout),\n",
        "                nn.Linear(self.hidden_sz * 2, self.hidden_sz),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(self.dropout),\n",
        "                nn.Linear(self.hidden_sz, self.n_classes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vgg16(x)"
      ],
      "metadata": {
        "id": "tav_mmAYM8LE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_uBfXeQOQT8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0e89c4-06ae-42c0-ab67-260e4cd5639f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add your path to the teacher model (download the teacher from github)"
      ],
      "metadata": {
        "id": "odEkeAY3c4Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = torch.load(\"/content/drive/MyDrive/vgg16_cifar10.pth\")"
      ],
      "metadata": {
        "id": "VLoIdfDTM0fD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c30b7b5-5758-4f7f-c577-5fdd0b61119c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-598a78d58e6f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher = torch.load(\"/content/drive/MyDrive/vgg16_cifar10.pth\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define model"
      ],
      "metadata": {
        "id": "prk1ihwMVbEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeiT(\n",
        "    image_sz=image_sz,\n",
        "    channel_sz=channel_sz,\n",
        "    patch_sz=patch_sz,\n",
        "    hidden_sz=hidden_sz,\n",
        "    embed_sz=embed_sz,\n",
        "    n_heads=n_heads,\n",
        "    n_layers=n_layers,\n",
        "    n_classes=n_classes,\n",
        "    teacher_model=teacher,\n",
        "    dropout=dropout\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "J28xeixPMgtA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Test"
      ],
      "metadata": {
        "id": "s0yviSzIT4Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, loss_function, optimizer, device, num_epochs):\n",
        "\n",
        "    training_history = {\n",
        "        \"accuracy\": [],\n",
        "        \"loss\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        true_labels = []\n",
        "        predicted_labels = []\n",
        "\n",
        "        for batch_index, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            student_predictions, teacher_predictions = model(images)\n",
        "            loss = loss_function(teacher_predictions, student_predictions, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            predicted_labels.extend(student_predictions.detach().argmax(dim=-1).tolist())\n",
        "            true_labels.extend(labels.detach().tolist())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        total_correct = sum(pred == true for pred, true in zip(predicted_labels, true_labels))\n",
        "        total_samples = len(predicted_labels)\n",
        "        accuracy = total_correct * 100 / total_samples\n",
        "\n",
        "        training_history[\"loss\"].append(epoch_loss)\n",
        "        training_history[\"accuracy\"].append(accuracy)\n",
        "\n",
        "        print(f\"{'-' * 50}\")\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        print(f\"Train Loss      : {epoch_loss:.6f}\")\n",
        "        print(f\"Train Accuracy  : {accuracy:.2f}% ({total_correct}/{total_samples})\")\n",
        "        print(f\"{'-' * 50}\")\n",
        "\n",
        "    return training_history"
      ],
      "metadata": {
        "id": "WnBXTU0iRso4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_index, (images, labels) in enumerate(test_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            predictions, _ = model(images)\n",
        "\n",
        "            predicted_labels.extend(predictions.argmax(dim=-1).tolist())\n",
        "            true_labels.extend(labels.tolist())\n",
        "\n",
        "    total_correct = sum(pred == true for pred, true in zip(predicted_labels, true_labels))\n",
        "    total_samples = len(predicted_labels)\n",
        "    accuracy = total_correct * 100 / total_samples\n",
        "\n",
        "    print(f\"{'-' * 50}\")\n",
        "    print(f\"Test Accuracy   : {accuracy:.2f}% ({total_correct}/{total_samples})\")\n",
        "    print(f\"{'-' * 50}\")\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "VM9O_pUpTtsf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocess cifar10"
      ],
      "metadata": {
        "id": "xpUO982bUEyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = datasets.CIFAR10('../data/CIFAR10/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_sz, shuffle=True)\n",
        "\n",
        "testset = datasets.CIFAR10('../data/CIFAR10/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_sz, shuffle=False)"
      ],
      "metadata": {
        "id": "Fqdoc_mkUGJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "898a8657-5d8d-4f96-9468-f7abb28c567d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/CIFAR10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/CIFAR10/cifar-10-python.tar.gz to ../data/CIFAR10/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## exec"
      ],
      "metadata": {
        "id": "iF2lKBEoT6ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = HardDistillationLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)"
      ],
      "metadata": {
        "id": "AJjsxOc7T3C8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, trainloader, criterion, optimizer, device, n_epochs)"
      ],
      "metadata": {
        "id": "ykkP6bEiT_Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, testloader, device)"
      ],
      "metadata": {
        "id": "KCp_5t2KXfUJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}